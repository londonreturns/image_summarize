from flask import Flask, request, jsonify
import os
import cv2
import numpy as np
from PIL import Image
import torch
import tempfile
import json
from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration

app = Flask(__name__)

# Set device
device = torch.device("cpu")
print(f"\nðŸ”§ Using device: {device}\n")

# Optional: torch performance tuning for CPU
torch.set_num_threads(os.cpu_count())
torch.backends.cudnn.benchmark = True

# Load model + processor once
print("ðŸ“¦ Loading model... (this may take a while)")
processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-flan-t5-xl")
model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-flan-t5-xl").to(device)
model.eval()
print("âœ… Model loaded!")

# Keyframe extraction
def get_keyframes(video_path, threshold=90):
    cap = cv2.VideoCapture(video_path)
    keyframes = []
    last_frame = None
    frame_id = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if last_frame is not None:
            diff = cv2.absdiff(gray, last_frame)
            non_zero_count = np.count_nonzero(diff)
            if non_zero_count > threshold * gray.size / 100:
                keyframes.append((frame_id, frame))

        last_frame = gray
        frame_id += 1

    cap.release()
    return keyframes

# Safe captioning wrapper
def caption_image(img: Image.Image, prompt: str):
    inputs = processor(images=img, text=prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=100)
    result = processor.tokenizer.decode(out[0], skip_special_tokens=True)
    
    try:
        return json.loads(result)
    except json.JSONDecodeError:
        return {"raw": result}

@app.route('/summarize', methods=['POST'])
def summarize_video():
    if 'video' not in request.files:
        return jsonify({"error": "No video uploaded"}), 400

    video_file = request.files['video']
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp:
        video_path = tmp.name
        video_file.save(video_path)

    frames = get_keyframes(video_path)
    os.remove(video_path)

    # Initialize categories
    people, actions, objects, scenes, summaries = [], [], [], [], []

    # Prompts
    prompts = {
        "people": "List all people or animals visible in this image as a JSON array.",
        "actions": "List all actions happening in this image as a JSON array.",
        "objects": "List all visible objects in this image as a JSON array.",
        "scenes": "Describe the setting or scene in this image.",
        "summaries": "Summarize what is happening in this frame."
    }

    print(f"ðŸ–¼ Extracted {len(frames)} keyframes.")

    for frame_id, frame in frames:
        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        # One pass per prompt
        people.append(caption_image(img, prompts["people"]))
        actions.append(caption_image(img, prompts["actions"]))
        objects.append(caption_image(img, prompts["objects"]))
        scenes.append(caption_image(img, prompts["scenes"]))
        summaries.append(caption_image(img, prompts["summaries"]))

    # Middle frame for overall summary
    mid_frame = frames[len(frames) // 2][1]
    overall_summary = caption_image(
        Image.fromarray(cv2.cvtColor(mid_frame, cv2.COLOR_BGR2RGB)),
        "Provide a complete JSON summary of the video with keys: 'people', 'actions', 'objects', 'scene'."
    )

    return jsonify({
        "people": people,
        "actions": actions,
        "objects": objects,
        "scenes": scenes,
        "frame_summaries": summaries,
        "overall_summary": overall_summary
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)
